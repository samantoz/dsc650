{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 6.2a\n",
    "Using section 5.2 in Deep Learning with Python as a guide, create a ConvNet model that classifies images CIFAR10 small images classification dataset\n",
    "\n",
    "-- Do not use dropout or data-augmentation in this part. \n",
    "\n",
    "-- Save the model, predictions, metrics, and validation plots in the dsc650/assignments/assignment06/results directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saman\\git_repos\\dsc650\\dsc650\\assignments\\assignment06\n",
      "c:\\Users\\saman\\git_repos\\dsc650\\dsc650\\assignments\\assignment06\\results\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "current_dir = Path(os.getcwd()).absolute()\n",
    "results_dir = current_dir.joinpath('results')\n",
    "\n",
    "print(current_dir)\n",
    "print(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# loading the required libraries and packages\n",
    "import sys\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "from keras.layers import Flatten\n",
    "\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Here we are using the CIFAR10 small images dataset to classify the images.\n",
    "\n",
    "This is a dataset of 50,000 32X32 color training images and 10,000 test images labeled over 10 categories.\n",
    "Each class is represented as a unique number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation is required before training the model\n",
    "def load_dataset():\n",
    "\n",
    "\t# loading the CIFAR10 dataset and create the training and test arrays\n",
    "\t(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "\t# Lines 1 and 2 reshapes the inputs\n",
    "\tX_train = X_train.reshape((X_train.shape[0], 32, 32, 3)).astype('float32')\n",
    "\tX_test = X_test.reshape((X_test.shape[0], 32, 32, 3)).astype('float32')\n",
    "\n",
    "\t# Lines 3 and 4 \n",
    "\t# Normalization of the input values (image pixels) from 0 and 255 to 0.1\n",
    "\tX_train = X_train / 255\n",
    "\tX_test = X_test / 255\n",
    "\n",
    "\t# Lines 5 and 6 \n",
    "\t# one-hot encoding of the target variables\n",
    "\ty_train = np_utils.to_categorical(y_train)\n",
    "\ty_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "\tnum_classes = y_test.shape[1]\n",
    "\n",
    "\treturn X_train, X_test, y_train, y_test\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model():\n",
    "\t# function to create the CNN model\n",
    "\t# Create model\n",
    "\tmodel = Sequential()  #model type is sequetial\n",
    "\t# Stacking convolutional layers with small 3 X 3 filters\n",
    "\t# It is followed by a max pooling layer.\n",
    "\t# Each of the above blocks are repeated where the number of filters in each block is increased.\n",
    "\t# Also the depth of the network such as 32,64 are also increased \n",
    "\t# Rectified Linear Activation ReLu is most widely used. It makes the network sparse and efficient \n",
    "\tmodel.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3), activation='relu'))\n",
    "\tmodel.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "\t# Adding the pooling layer\n",
    "\tmodel.add(MaxPooling2D(2,2))\n",
    "\n",
    "\tmodel.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\tmodel.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\t# Adding the pooling layer\n",
    "\tmodel.add(MaxPooling2D(2,2))\n",
    "\n",
    "\tmodel.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "\tmodel.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "\t# Adding the pooling layer\n",
    "\tmodel.add(MaxPooling2D(2, 2))\n",
    "\n",
    "\t# Flatten layer converts the 2D matrix data to a vector\n",
    "\tmodel.add(Flatten())\n",
    "\t# Fully connected dense layer with 128 neurons\n",
    "\tmodel.add(Dense(128, activation='relu'))\n",
    "\t# output layer which has 10 neurons for the 10 classes\n",
    "\tmodel.add(Dense(num_classes, activation='softmax'))\n",
    "\t\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "def summary_plot(history):\n",
    "\n",
    "\tacc = history.history['accuracy']\n",
    "\tval_acc = history.history['val_accuracy']\n",
    "\tloss = history.history['loss']\n",
    "\tval_loss = history.history['val_loss']\n",
    "\t\t\n",
    "\tepochs = range(1, len(acc) + 1)\n",
    "\n",
    "\tplt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "\tplt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "\tplt.title('Training and validation accuracy')\n",
    "\tplt.legend()\n",
    "\n",
    "\tplt.figure()\n",
    "\n",
    "\tplt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "\tplt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "\tplt.title('Training and validation loss')\n",
    "\tplt.legend()\n",
    "\n",
    "\tplt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(history):\n",
    "\tplt.subplot(211)\n",
    "\tplt.title('Cross Entropy Loss')\n",
    "\tplt.plot(history.history['loss'], color='blue', label='train')\n",
    "\tplt.plot(history.history['val_loss'], color='orange', label='test')\n",
    "\t# plot accuracy\n",
    "\tplt.subplot(212)\n",
    "\tplt.title('Classification Accuracy')\n",
    "\tplt.plot(history.history['accuracy'], color='blue', label='train')\n",
    "\tplt.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "\t# save plot to file\n",
    "\tplt.savefig(f'{results_dir}\\\\1_plot.png')\n",
    "\tplt.show()\n",
    "\tplt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dataset()\n",
    "model = cnn_model()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=150, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "plt.title('Cross Entropy Loss')\n",
    "plt.plot(history.history['loss'], color='blue', label='train')\n",
    "plt.plot(history.history['val_loss'], color='orange', label='test')\n",
    "# plot accuracy\n",
    "plt.subplot(212)\n",
    "plt.title('Classification Accuracy')\n",
    "plt.plot(history.history['accuracy'], color='blue', label='train')\n",
    "plt.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "# save plot to file\n",
    "plt.savefig(f'{results_dir}\\\\1_plot.png')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"CNN Accuracy: %.3f%%\" % (scores[1]*100.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model():\n",
    "\t\n",
    "\tprint('Load dataset')\n",
    "\tload_dataset()\n",
    "\t\n",
    "\tprint('dataset loaded')\n",
    "\tprint(f'Training set: {X_train.shape}')\n",
    "\tprint(f'Test Set: {X_test.shape}')\n",
    "\tprint(f'Number of categories : {num_classes}')\n",
    "\n",
    "\tprint('Build model')\n",
    "\tmodel = cnn_model()\n",
    "\tprint('Model is defined')\n",
    "\tprint('Summary of the model.')\n",
    "\tmodel.summary()\n",
    "\n",
    "\tprint('Compile Model')\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\tprint('Model compiled')\n",
    "\n",
    "\tprint('Model fitting Considering 5 epochs and a batch size of 150')\n",
    "\thistory = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=150, verbose=0)\n",
    "\t\n",
    "\tprint('Saving the model')\n",
    "\tmodel.save(f'{results_dir}\\\\assignment_6.2a_cifar10.h5')\n",
    "\t\n",
    "\tprint('Evaluating the model on the test data')\n",
    "\tscores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "\tprint(\"CNN Accuracy: %.3f%%\" % (scores[1]*100.0))\n",
    "\t\t\n",
    "\tprint('Output summary')\n",
    "\t# summary_plot(history)\n",
    "\tsummarize_diagnostics(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load dataset\n",
      "dataset loaded\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-d8af0fd933a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-825cddb66b4a>\u001b[0m in \u001b[0;36mrun_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dataset loaded'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training set: {X_train.shape}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Test Set: {X_test.shape}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Number of categories : {num_classes}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "run_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape of the second image from the training set\n",
    "X_train[1].shape\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first few iamges\n",
    "for i in range(9):\n",
    "\t# define the subplot\n",
    "\tplt.subplot(330 + 1 + i)\n",
    "\t# plot the data\n",
    "\tplt.imshow(X_train[i])\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We see from the summary report that our input (5, 5, 128) outputs were flattened into vectors of shape (3200,), before going through two Dense layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting and Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above output shows that with only five epochs we have achieved accuracy of 98.73% on our validation data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These plots are characteristics of overfitting. The training accuracy increases until it reaches 100% whereas the validation accuracy stalls a little above 99%.\n",
    "#### The validation loss reaches its minimum around 3 epochs and then stays almost the same, whereas the training loss keeps decreasing until it reaches nearly 0"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d18a9889dea234ea0a2ae94165273cf00045647714b4daa7d07ae97b3e052eb7"
  },
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('BU_DSC': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
