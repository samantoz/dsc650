{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 11\n",
    "Using section 8.1 in Deep Learning with Python as a guide, implement an LSTM text generator. Train the model on the Enron corpus or a text source of your choice. Save the model and generate 20 examples to the results directory of dsc650/assignments/assignment11/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saman\\git_repos\\dsc650\\dsc650\\assignments\\assignments11\n",
      "c:\\Users\\saman\\git_repos\\dsc650\\dsc650\\assignments\\assignments11\\results\n",
      "c:\\Users\\saman\\git_repos\\dsc650\\dsc650\\assignments\\assignments11\\data\\corpus\n"
     ]
    }
   ],
   "source": [
    "current_dir = Path(os.getcwd()).absolute()\n",
    "results_dir = current_dir.joinpath('results')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "data_dir = current_dir.joinpath('data')\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "corpus_data_dir = data_dir.joinpath('corpus')\n",
    "\n",
    "print(current_dir)\n",
    "print(results_dir)\n",
    "print(corpus_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded into:  C:\\Users\\saman\\.keras\\datasets\\3090-0.txt\n",
      "Corpus length: 2730110\n"
     ]
    }
   ],
   "source": [
    "# Downloading and parsing the initial text\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "path = keras.utils.get_file(\n",
    "\t'3090-0.txt',\n",
    "\torigin='https://www.gutenberg.org/files/3090/3090-0.txt')\n",
    "print('Downloaded into: ', path)\n",
    "text = open(path,encoding = 'utf-8').read().lower()\n",
    "print('Corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 910017\n"
     ]
    }
   ],
   "source": [
    "# Vectorizing sequences of characters\n",
    "\n",
    "maxlen = 60 # extract sequences of 60 characters\n",
    "step = 3 # sample a new sequence every 3 characters\n",
    "sentences = [] # holds the extracted sequences\n",
    "next_chars = [] # holds the targets (in this case the next character)\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "\tsentences.append(text[i: i + maxlen])\n",
    "\tnext_chars.append(text[i + maxlen])\n",
    "print('Number of sequences:', len(sentences) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeffthe project gutenberg ebook of original short stories, comp',\n",
       " 'e project gutenberg ebook of original short stories, complet',\n",
       " 'roject gutenberg ebook of original short stories, complete, ',\n",
       " 'ect gutenberg ebook of original short stories, complete, by ',\n",
       " ' gutenberg ebook of original short stories, complete, by guy',\n",
       " 'tenberg ebook of original short stories, complete, by guy de',\n",
       " 'berg ebook of original short stories, complete, by guy de ma',\n",
       " 'g ebook of original short stories, complete, by guy de maupa',\n",
       " 'book of original short stories, complete, by guy de maupassa',\n",
       " 'k of original short stories, complete, by guy de maupassant\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['l', 'e', 'b', 'g', ' ', ' ', 'u', 's', 'n', '\\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_chars[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters: 60\n"
     ]
    }
   ],
   "source": [
    "# list of unique characters in the corpus\n",
    "chars = sorted(list(set(text)))\n",
    "print('Unique characters:', len(chars))\n",
    "# Dictionary that maps unique characters to their index in the list \"chars\"\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n     !  \"  #  $  %  \\'  (  )  *  ,  -  .  /  0  1  2  3  4  5  6  7  8  9  :  ;  ?  [  ]  a  b  c  d  e  f  g  h  i  j  k  l  m  n  o  p  q  r  s  t  u  v  w  x  y  z  —  “  ”  \\ufeff'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'  '.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '\"': 3,\n",
       " '#': 4,\n",
       " '$': 5,\n",
       " '%': 6,\n",
       " \"'\": 7,\n",
       " '(': 8,\n",
       " ')': 9,\n",
       " '*': 10,\n",
       " ',': 11,\n",
       " '-': 12,\n",
       " '.': 13,\n",
       " '/': 14,\n",
       " '0': 15,\n",
       " '1': 16,\n",
       " '2': 17,\n",
       " '3': 18,\n",
       " '4': 19,\n",
       " '5': 20,\n",
       " '6': 21,\n",
       " '7': 22,\n",
       " '8': 23,\n",
       " '9': 24,\n",
       " ':': 25,\n",
       " ';': 26,\n",
       " '?': 27,\n",
       " '[': 28,\n",
       " ']': 29,\n",
       " 'a': 30,\n",
       " 'b': 31,\n",
       " 'c': 32,\n",
       " 'd': 33,\n",
       " 'e': 34,\n",
       " 'f': 35,\n",
       " 'g': 36,\n",
       " 'h': 37,\n",
       " 'i': 38,\n",
       " 'j': 39,\n",
       " 'k': 40,\n",
       " 'l': 41,\n",
       " 'm': 42,\n",
       " 'n': 43,\n",
       " 'o': 44,\n",
       " 'p': 45,\n",
       " 'q': 46,\n",
       " 'r': 47,\n",
       " 's': 48,\n",
       " 't': 49,\n",
       " 'u': 50,\n",
       " 'v': 51,\n",
       " 'w': 52,\n",
       " 'x': 53,\n",
       " 'y': 54,\n",
       " 'z': 55,\n",
       " '—': 56,\n",
       " '“': 57,\n",
       " '”': 58,\n",
       " '\\ufeff': 59}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "\tfor t, char in enumerate(sentence):\n",
    "\t\tx[i, t, char_indices[char]] = 1\n",
    "\ty[i, char_indices[next_chars[i]]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single layer LSTM model for next-character prediction\n",
    "# Tis network is a single LSTM layer followed by a Dense classifier and softmax over all possible characters\n",
    "from keras import layers\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model compilation configuration\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d2398cd6c937708e42b80c0dce25ca405d0c6827c5ebcbcac9817d73cd0bc02f"
  },
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('dsc650': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
